grid_size: 5  # Overriding the default grid size to 15x15
agents: [2,1]  # 4 agents of type 0 and 3 agents of type 1
prey_predator_combo:
  0: 1  # Type 1 can capture Type 0
  1: None  # Type 0 can capture Type 1
reward:
  win: 1      # Increased reward for winning
  lose: -1      # Increased penalty for losing
  captured: 1    # Increased reward for capturing
  eliminated: -1  # Increased penalty for elimination
  step: -0.1        # Increased penalty for each step
  prey_step: 0.1   # Increased penalty for each step of the prey
  distance: -0.1    # Increased penalty for each unit distance
  boundary: -0.1    # Increased penalty for hitting the boundary
animation: True  # Disable animation
fps: 3  # Faster frame rate for animation
position_random: True  # Disable random positions
capture: False  # Prey is eliminated, not captured
# Training Hyperparameters
n_actions: 4
num_episodes: 100
max_steps: 15
model: "dqn"
hidden_dim: [64, 128, 256, 128, 64]
save_path: "results/" # Put the absolute path to the experiments folder
tag: "trial_57"
config_file: "src/config.yaml"  # Path to the custom config file
smooth_plot: True  # Smooth the training plot
ani_batch_size: 50  # Batch size for animation
distance: True
distance_type: "average" # "average" or "last"
max_step_ani: True # If you want to animate the max step
boundary: True
distance_input: True
experience_replay: True
batch_size: 32