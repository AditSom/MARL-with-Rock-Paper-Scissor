{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/haoyuluo/Desktop/Columbia_MS/EE/ELENE6885_001_2024_3 - TOPICS IN SIGNAL PROCESSING/Final_Project/MARL-with-Rock-Paper-Scissor/src\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'my_env'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 10\u001b[0m\n\u001b[1;32m      7\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/haoyuluo/Desktop/Columbia_MS/EE/ELENE6885_001_2024_3 - TOPICS IN SIGNAL PROCESSING/Final_Project/MARL-with-Rock-Paper-Scissor/src\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent working directory:\u001b[39m\u001b[38;5;124m\"\u001b[39m, os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m  \u001b[38;5;21;01mmy_gym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config, Environment\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrandom\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/Columbia_MS/EE/ELENE6885_001_2024_3 - TOPICS IN SIGNAL PROCESSING/Final_Project/MARL-with-Rock-Paper-Scissor/src/my_gym/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Config  \u001b[38;5;66;03m# Import Config class\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmy_env\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menvironment\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Environment  \u001b[38;5;66;03m# Import Environment class\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Define what gets imported when the package is imported\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'my_env'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "# Add the src folder to Python's module search path\n",
    "\n",
    "# Navigate to the `src` folder\n",
    "os.chdir('/Users/haoyuluo/Desktop/Columbia_MS/EE/ELENE6885_001_2024_3 - TOPICS IN SIGNAL PROCESSING/Final_Project/MARL-with-Rock-Paper-Scissor/src')\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "from  my_gym import Config, Environment\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN for Agent Type 0\n",
    "class DQNType0(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNType0, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,))\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)\n",
    "\n",
    "# DQN for Agent Type 1\n",
    "class DQNType1(tf.keras.Model):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQNType1, self).__init__()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, activation='relu', input_shape=(input_dim,))\n",
    "        self.fc2 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.fc3 = tf.keras.layers.Dense(output_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent Class for Type 0\n",
    "class AgentType0:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.grid_size = env.grid_size\n",
    "        self.n_actions = 4  # Up, Down, Left, Right\n",
    "\n",
    "        self.input_dim = self.grid_size * self.grid_size  # Flattened grid\n",
    "\n",
    "        # DQN networks\n",
    "        self.q_network = DQNType0(input_dim=self.input_dim, output_dim=self.n_actions)\n",
    "        self.target_network = DQNType0(input_dim=self.input_dim, output_dim=self.n_actions)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "            q_values = self.q_network(state)\n",
    "            return np.argmax(q_values.numpy()[0])\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # Idea similar to 1predator_1prey.update_q_network\n",
    "    def update_q_network(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states).astype(np.float32)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards).astype(np.float32)\n",
    "        next_states = np.array(next_states).astype(np.float32)\n",
    "        dones = np.array(dones).astype(np.float32)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_network(next_states)\n",
    "        max_next_q_values = np.max(next_q_values.numpy(), axis=1)\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        # Compute current Q-values\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states)\n",
    "            indices = np.arange(self.batch_size)\n",
    "            action_q_values = tf.gather_nd(q_values, np.vstack([indices, actions]).T)\n",
    "            loss = self.loss_fn(target_q_values, action_q_values)\n",
    "\n",
    "        # Update network parameters\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "        \n",
    "    # Agent Class for Type 1 (similar to Type 0)\n",
    "class AgentType1:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.grid_size = env.grid_size\n",
    "        self.n_actions = 4  # Up, Down, Left, Right\n",
    "\n",
    "        self.input_dim = self.grid_size * self.grid_size  # Flattened grid\n",
    "\n",
    "        # DQN networks\n",
    "        self.q_network = DQNType1(input_dim=self.input_dim, output_dim=self.n_actions)\n",
    "        self.target_network = DQNType1(input_dim=self.input_dim, output_dim=self.n_actions)\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "\n",
    "        # Optimizer and loss\n",
    "        self.optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "        self.loss_fn = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "        # Replay buffer\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, self.n_actions - 1)\n",
    "        else:\n",
    "            state = np.expand_dims(state, axis=0).astype(np.float32)\n",
    "            q_values = self.q_network(state)\n",
    "            return np.argmax(q_values.numpy()[0])\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update_q_network(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.array(states).astype(np.float32)\n",
    "        actions = np.array(actions)\n",
    "        rewards = np.array(rewards).astype(np.float32)\n",
    "        next_states = np.array(next_states).astype(np.float32)\n",
    "        dones = np.array(dones).astype(np.float32)\n",
    "\n",
    "        # Compute target Q-values\n",
    "        next_q_values = self.target_network(next_states)\n",
    "        max_next_q_values = np.max(next_q_values.numpy(), axis=1)\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * max_next_q_values\n",
    "\n",
    "        # Compute current Q-values\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.q_network(states)\n",
    "            indices = np.arange(self.batch_size)\n",
    "            action_q_values = tf.gather_nd(q_values, np.vstack([indices, actions]).T)\n",
    "            loss = self.loss_fn(target_q_values, action_q_values)\n",
    "\n",
    "        # Update network parameters\n",
    "        grads = tape.gradient(loss, self.q_network.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.q_network.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.q_network.get_weights())\n",
    "        \n",
    "def get_agent_state(env, agent_id):\n",
    "    # Use the full grid as the observation\n",
    "    grid = env.grid.copy()\n",
    "    state = grid.flatten()\n",
    "    return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m      2\u001b[0m     config_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfig.yaml\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[43mConfig\u001b[49m(config_file)\n\u001b[1;32m      4\u001b[0m     env \u001b[38;5;241m=\u001b[39m Environment(config)\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Load configuration\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Config' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    config_file = \"config.yaml\"\n",
    "    config = Config(config_file)\n",
    "    env = Environment(config)\n",
    "\n",
    "    # Load configuration\n",
    "    config = Config(\"mypackage/config.yaml\")  # Path to your YAML file\n",
    "    env = Environment(config)  # Initialize the environment\n",
    "\n",
    "    # Number of steps in one episode\n",
    "    # max_steps = 50  # Adjust based on the problem\n",
    "\n",
    "    # Run one episode\n",
    "    print(\"Initial Environment:\")\n",
    "    env.render()\n",
    "    \n",
    "# Initialize agents\n",
    "    agent_type0 = AgentType0(env)\n",
    "    agent_type1 = AgentType1(env)\n",
    "\n",
    "    # Map agent IDs to their types\n",
    "    agent_id_to_type = {}\n",
    "    for idx, pos in enumerate(env.positions):\n",
    "        agent_id_to_type[idx] = pos['agent']\n",
    "\n",
    "    num_episodes = 1000\n",
    "    target_update_freq = 10\n",
    "    max_steps_per_episode = 100\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    total_rewards = {0: 0, 1: 0}\n",
    "    done = False\n",
    "    timestep = 0\n",
    "\n",
    "    while not done and timestep < max_steps_per_episode:\n",
    "        actions = []\n",
    "        states = []  # List to store states before actions\n",
    "        # Each agent selects an action\n",
    "        for agent_id in range(env.total_agents):\n",
    "            if agent_id not in env.captured_agents:\n",
    "                agent_type = env.positions[agent_id]['agent']\n",
    "                state = get_agent_state(env, agent_id)\n",
    "                states.append(state)  # Store the state before action\n",
    "                if agent_type == 0:\n",
    "                    action = agent_type0.select_action(state)\n",
    "                elif agent_type == 1:\n",
    "                    action = agent_type1.select_action(state)\n",
    "                else:\n",
    "                    action = random.randint(0, 3)  # Default random action\n",
    "            else:\n",
    "                action = None  # Agent has been captured\n",
    "                states.append(None)  # Placeholder for consistency\n",
    "            actions.append(action)\n",
    "\n",
    "        # Environment processes all actions\n",
    "        env.step(actions, timestep)\n",
    "        timestep += 1\n",
    "\n",
    "        # Update each agent\n",
    "        for agent_id in range(env.total_agents):\n",
    "            if agent_id not in env.captured_agents:\n",
    "                agent_type = env.positions[agent_id]['agent']\n",
    "                state = states[agent_id]  # State before action\n",
    "                reward = env.reward[agent_type]\n",
    "                done = env.done\n",
    "                next_state = get_agent_state(env, agent_id)  # State after action\n",
    "\n",
    "                if agent_type == 0:\n",
    "                    agent_type0.store_transition(state, actions[agent_id], reward, next_state, done)\n",
    "                    agent_type0.update_q_network()\n",
    "                    total_rewards[0] += reward\n",
    "                elif agent_type == 1:\n",
    "                    agent_type1.store_transition(state, actions[agent_id], reward, next_state, done)\n",
    "                    agent_type1.update_q_network()\n",
    "                    total_rewards[1] += reward\n",
    "\n",
    "        if env.done:\n",
    "            break\n",
    "\n",
    "    # Update target networks and decay epsilon\n",
    "    if episode % target_update_freq == 0:\n",
    "        agent_type0.update_target_network()\n",
    "        agent_type1.update_target_network()\n",
    "\n",
    "    if agent_type0.epsilon > agent_type0.epsilon_min:\n",
    "        agent_type0.epsilon *= agent_type0.epsilon_decay\n",
    "    if agent_type1.epsilon > agent_type1.epsilon_min:\n",
    "        agent_type1.epsilon *= agent_type1.epsilon_decay\n",
    "\n",
    "    print(f\"Episode {episode + 1}/{num_episodes}, Total Rewards: {total_rewards}\")\n",
    "\n",
    "\n",
    "    # for step in range(max_steps):\n",
    "    #     if env.done:\n",
    "    #         print(f\"Episode ended after {step} steps.\")\n",
    "    #         break\n",
    "\n",
    "    #     # Select random actions for all agents\n",
    "    #     action_list = random_action_selector(env.total_agents)\n",
    "        \n",
    "    #     # Take a step in the environment\n",
    "    #     env.step(action_list)\n",
    "        \n",
    "    #     # Render the environment\n",
    "    #     print(f\"Step {step + 1}:\")\n",
    "\n",
    "    # Check results\n",
    "    # if env.done:\n",
    "    #     print(f\"Winner: Agent Type {env.winner}\")\n",
    "    # else:\n",
    "    #     print(\"Episode ended without a winner.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new183",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
