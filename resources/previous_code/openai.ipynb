{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreyPredatorEnv(gym.Env):\n",
    "    def __init__(self, num_predators=2, num_preys=2):\n",
    "        super(PreyPredatorEnv, self).__init__()\n",
    "        self.grid_size = 5\n",
    "        self.num_predators = num_predators\n",
    "        self.num_preys = num_preys\n",
    "        self.action_space = spaces.MultiDiscrete([4] * (num_predators + num_preys))  # Actions for all predators and preys\n",
    "        self.observation_space = spaces.Box(low=0, high=self.grid_size-1, shape=(2 * (num_predators + num_preys),), dtype=np.int32)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # Predators start at random positions\n",
    "        self.predator_positions = np.random.randint(0, self.grid_size, size=(self.num_predators, 2))\n",
    "        # Preys start at random positions\n",
    "        self.prey_positions = np.random.randint(0, self.grid_size, size=(self.num_preys, 2))\n",
    "        return np.concatenate([self.predator_positions.flatten(), self.prey_positions.flatten()])\n",
    "\n",
    "    def step(self, predator_actions, prey_actions):\n",
    "        # predator_actions = actions[:self.num_predators]\n",
    "        # prey_actions = actions[self.num_predators:]\n",
    "\n",
    "        # Move the predators based on their actions\n",
    "        print(predator_actions)\n",
    "        for i, action in enumerate(predator_actions):\n",
    "            if action == 0:  # up\n",
    "                self.predator_positions[i][0] = max(self.predator_positions[i][0] - 1, 0)\n",
    "            elif action == 1:  # down\n",
    "                self.predator_positions[i][0] = min(self.predator_positions[i][0] + 1, self.grid_size - 1)\n",
    "            elif action == 2:  # left\n",
    "                self.predator_positions[i][1] = max(self.predator_positions[i][1] - 1, 0)\n",
    "            elif action == 3:  # right\n",
    "                self.predator_positions[i][1] = min(self.predator_positions[i][1] + 1, self.grid_size - 1)\n",
    "\n",
    "        # Move the preys based on their actions\n",
    "        for i, action in enumerate(prey_actions):\n",
    "            if action == 0:  # up\n",
    "                self.prey_positions[i][0] = max(self.prey_positions[i][0] - 1, 0)\n",
    "            elif action == 1:  # down\n",
    "                self.prey_positions[i][0] = min(self.prey_positions[i][0] + 1, self.grid_size - 1)\n",
    "            elif action == 2:  # left\n",
    "                self.prey_positions[i][1] = max(self.prey_positions[i][1] - 1, 0)\n",
    "            elif action == 3:  # right\n",
    "                self.prey_positions[i][1] = min(self.prey_positions[i][1] + 1, self.grid_size - 1)\n",
    "\n",
    "        # Check if any predator caught any prey\n",
    "        done = False\n",
    "        predator_reward = -0.1 * self.num_predators\n",
    "        prey_reward = 0\n",
    "        for predator_pos in self.predator_positions:\n",
    "            for prey_pos in self.prey_positions:\n",
    "                if np.array_equal(predator_pos, prey_pos):\n",
    "                    done = True\n",
    "                    predator_reward += 1\n",
    "                    prey_reward -= 10\n",
    "\n",
    "        return np.concatenate([self.predator_positions.flatten(), self.prey_positions.flatten()]), predator_reward, prey_reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.set_xlim(0, self.grid_size)\n",
    "        ax.set_ylim(0, self.grid_size)\n",
    "        ax.set_xticks(np.arange(0, self.grid_size, 1))\n",
    "        ax.set_yticks(np.arange(0, self.grid_size, 1))\n",
    "        ax.grid(True)\n",
    "\n",
    "        # Add preys\n",
    "        for prey_pos in self.prey_positions:\n",
    "            prey_patch = patches.Rectangle(prey_pos[::-1], 1, 1, edgecolor='green', facecolor='green', label=\"Prey\")\n",
    "            ax.add_patch(prey_patch)\n",
    "\n",
    "        # Add predators\n",
    "        for predator_pos in self.predator_positions:\n",
    "            predator_patch = patches.Rectangle(predator_pos[::-1], 1, 1, edgecolor='red', facecolor='red', label=\"Predator\")\n",
    "            ax.add_patch(predator_patch)\n",
    "\n",
    "        plt.legend(handles=[prey_patch, predator_patch])\n",
    "        plt.title('Predator-Prey Grid World')\n",
    "        plt.pause(0.3)  # Pause to visualize each step\n",
    "        plt.show()\n",
    "\n",
    "    def close(self):\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural Network for the Q-value function\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, env, input_dim=4):\n",
    "        self.env = env\n",
    "        self.input_dim = input_dim\n",
    "        # print(env.action_space)\n",
    "        self.q_network = DQN(input_dim=self.input_dim, output_dim=8)\n",
    "        self.target_network = DQN(input_dim=self.input_dim, output_dim=8)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=0.001)\n",
    "        self.loss_fn = nn.MSELoss()\n",
    "\n",
    "        self.memory = deque(maxlen=10000)\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.epsilon = 0.1  # Exploration rate\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.001\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0)\n",
    "                q_values = self.q_network(state)\n",
    "                return q_values.argmax().item()\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def update_q_network(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "\n",
    "        current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        next_q_values = self.target_network(next_states).max(1)[0]\n",
    "        target_q_values = rewards + (1 - dones) * self.gamma * next_q_values\n",
    "\n",
    "        loss = self.loss_fn(current_q_values, target_q_values)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\adits\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m\n\u001b[0;32m     18\u001b[0m predator_action \u001b[38;5;241m=\u001b[39m predator\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[0;32m     19\u001b[0m prey_action \u001b[38;5;241m=\u001b[39m prey\u001b[38;5;241m.\u001b[39mselect_action(state)\n\u001b[1;32m---> 20\u001b[0m next_state, predator_reward, prey_reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredator_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprey_action\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m predator\u001b[38;5;241m.\u001b[39mstore_transition(state, predator_action, predator_reward, next_state, done)\n\u001b[0;32m     22\u001b[0m prey\u001b[38;5;241m.\u001b[39mstore_transition(state, prey_action, prey_reward, next_state, done)\n",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m, in \u001b[0;36mPreyPredatorEnv.step\u001b[1;34m(self, predator_actions, prey_actions)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, predator_actions, prey_actions):\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# predator_actions = actions[:self.num_predators]\u001b[39;00m\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;66;03m# prey_actions = actions[self.num_predators:]\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Move the predators based on their actions\u001b[39;00m\n\u001b[0;32m     23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(predator_actions)\n\u001b[1;32m---> 24\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpredator_actions\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     25\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m action \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:  \u001b[38;5;66;03m# up\u001b[39;00m\n\u001b[0;32m     26\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredator_positions[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredator_positions[i][\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "env = PreyPredatorEnv()\n",
    "predator = Agent(env, 8)\n",
    "prey = Agent(env, 8)\n",
    "num_episodes = 5000\n",
    "\n",
    "predator_rewards_per_episode = []\n",
    "prey_rewards_per_episode = []\n",
    "steps_per_episode = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    predator_total_reward = 0\n",
    "    prey_total_reward = 0\n",
    "    steps = 0\n",
    "    done = False\n",
    "\n",
    "    while steps < 100 and not done:\n",
    "        predator_action = predator.select_action(state)\n",
    "        prey_action = prey.select_action(state)\n",
    "        next_state, predator_reward, prey_reward, done, _ = env.step(predator_action, prey_action)\n",
    "        predator.store_transition(state, predator_action, predator_reward, next_state, done)\n",
    "        prey.store_transition(state, prey_action, prey_reward, next_state, done)\n",
    "        predator.update_q_network()\n",
    "        prey.update_q_network()\n",
    "        state = next_state\n",
    "        predator_total_reward += predator_reward\n",
    "        prey_total_reward += prey_reward\n",
    "        steps += 1\n",
    "\n",
    "    predator.update_target_network()\n",
    "    prey.update_target_network()\n",
    "\n",
    "    if predator.epsilon > predator.epsilon_min:\n",
    "        predator.epsilon *= predator.epsilon_decay\n",
    "    \n",
    "    if prey.epsilon > prey.epsilon_min:\n",
    "        prey.epsilon *= prey.epsilon_decay\n",
    "\n",
    "    predator_rewards_per_episode.append(predator_total_reward)\n",
    "    prey_rewards_per_episode.append(prey_total_reward)\n",
    "    steps_per_episode.append(steps)\n",
    "\n",
    "    print(f\"Episode {episode + 1}, Total Reward for Predator: {predator_total_reward}, Total Reward for Prey: {prey_total_reward}, Steps: {steps}\")\n",
    "\n",
    "# Plot the learning process\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot total reward per episode for predator\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(predator_rewards_per_episode, label='Predator')\n",
    "plt.plot(prey_rewards_per_episode, label='Prey')\n",
    "plt.title('Total Reward per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.legend()\n",
    "\n",
    "# Plot number of steps per episode\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(steps_per_episode)\n",
    "plt.title('Steps per Episode')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Number of Steps')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# # Visualize the strategy learned by the agent in a test run\n",
    "# state = env.reset()\n",
    "# done = False\n",
    "# s = 0\n",
    "# while s < 50 and not done:\n",
    "#     s += 1\n",
    "#     env.render()  # Render each step\n",
    "#     predator_action = predator.select_action(state)  # Use the learned policy\n",
    "#     prey_action = prey.select_action(state)\n",
    "#     state, predator_reward, prey_reward, done, _ = env.step(predator_action, prey_action)\n",
    "# env.render()  # Render the final state\n",
    "# env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode: 0\n",
      "2220\n",
      "4\n",
      "Current episode: 10\n",
      "Current episode: 20\n",
      "2508\n",
      "913\n",
      "Current episode: 30\n",
      "7836\n",
      "734\n",
      "Current episode: 40\n",
      "1373\n",
      "Current episode: 50\n",
      "7642\n",
      "Current episode: 60\n",
      "Current episode: 70\n",
      "Current episode: 80\n",
      "Current episode: 90\n",
      "Total kill: 8/100\n",
      "Average steps: 9432.3\n"
     ]
    }
   ],
   "source": [
    "# test score\n",
    "num_episodes = 100\n",
    "predator_total_reward = 0\n",
    "prey_total_reward = 0\n",
    "tot_steps = 0\n",
    "count_kill = 0\n",
    "for episode in range(num_episodes):\n",
    "    if episode % 10 == 0:\n",
    "        print(\"Current episode:\", episode)\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    steps = 0\n",
    "    while steps < 10000 and not done:\n",
    "        predator_action = predator.select_action(state)\n",
    "        prey_action = prey.select_action(state)\n",
    "        next_state, predator_reward, prey_reward, done, _ = env.step(predator_action, prey_action)\n",
    "        state = next_state\n",
    "        predator_total_reward += predator_reward\n",
    "        prey_total_reward += prey_reward\n",
    "        steps += 1\n",
    "    \n",
    "    if done:\n",
    "        count_kill += 1\n",
    "        print(steps)\n",
    "    \n",
    "    tot_steps += steps\n",
    "\n",
    "print(f\"Total kill: {count_kill}/{num_episodes}\")\n",
    "print(f\"Average steps: {tot_steps/num_episodes}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
